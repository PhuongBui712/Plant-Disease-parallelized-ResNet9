{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from numba import cuda, float32\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Function\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "from numba import cuda\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def batchnorm2d_forward_kernel(input, output, mean, inv_std, gamma, beta):\n",
    "    idx, out_h, out_w = cuda.grid(3)\n",
    "\n",
    "    batch_idx = idx // input.shape[1]\n",
    "    channel = idx % input.shape[1]\n",
    "\n",
    "    if batch_idx < output.shape[0] and channel < output.shape[1] and out_h < output.shape[2] and out_w < output.shape[3]:\n",
    "        normalized = (input[batch_idx, channel, out_h, out_w] - mean[channel]) * inv_std[channel]\n",
    "        output[batch_idx, channel, out_h, out_w] = normalized * gamma[channel] + beta[channel]\n",
    "\n",
    "\n",
    "class NumbaBatchNorm2dFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,\n",
    "                input: Tensor,\n",
    "                gamma: Tensor, \n",
    "                beta: Tensor, \n",
    "                running_mean: Optional[Tensor], \n",
    "                running_var: Optional[Tensor], \n",
    "                eps: float, \n",
    "                momentum: float, \n",
    "                training: bool) -> Tensor:\n",
    "        input = input.contiguous()\n",
    "        \n",
    "        if training:\n",
    "            mean = input.mean(dim=(0, 2, 3))\n",
    "            var = input.var(dim=(0, 2, 3), unbiased=False)\n",
    "            \n",
    "            if running_mean is not None:\n",
    "                running_mean.mul_(1 - momentum).add_(mean * momentum)\n",
    "            if running_var is not None:\n",
    "                running_var.mul_(1 - momentum).add_(var * momentum)\n",
    "        else:\n",
    "            mean = running_mean\n",
    "            var = running_var\n",
    "        \n",
    "        inv_std = 1 / torch.sqrt(var + eps)\n",
    "        output = torch.empty_like(input)\n",
    "        \n",
    "        threads_per_block = (8, 8, 8)\n",
    "        blocks_per_grid = (\n",
    "            math.ceil(input.shape[0] * input.shape[1] / threads_per_block[0]),\n",
    "            math.ceil(input.shape[2] / threads_per_block[1]),\n",
    "            math.ceil(input.shape[3] / threads_per_block[2])\n",
    "        )\n",
    "\n",
    "        batchnorm2d_forward_kernel[blocks_per_grid, threads_per_block](\n",
    "            input.detach(), output, mean.detach(), inv_std.detach(), gamma.detach(), beta.detach()\n",
    "        )\n",
    "        \n",
    "        ctx.save_for_backward(input, gamma, mean, inv_std)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor) -> Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor], None, None, None, None, None]:\n",
    "        input, gamma, mean, inv_std = ctx.saved_tensors\n",
    "        \n",
    "        # Use PyTorch's built-in backward pass for simplicity and correctness\n",
    "        normalized = (input - mean[None, :, None, None]) * inv_std[None, :, None, None]\n",
    "        grad_input = F.batch_norm(\n",
    "            input, mean, 1/inv_std**2, gamma, None, \n",
    "            eps=0, momentum=0, training=True\n",
    "        )\n",
    "        grad_input = grad_output * grad_input\n",
    "        \n",
    "        grad_gamma = (grad_output * normalized).sum(dim=(0, 2, 3))\n",
    "        grad_beta = grad_output.sum(dim=(0, 2, 3))\n",
    "        \n",
    "        return grad_input, grad_gamma, grad_beta, None, None, None, None, None\n",
    "\n",
    "\n",
    "class NumbaBatchNorm2d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 eps: float = 1e-05,\n",
    "                 momentum: float = 0.1,\n",
    "                 affine: bool = True,\n",
    "                 track_running_stats: bool = True) -> None:\n",
    "                 \n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        \n",
    "        if self.affine:\n",
    "            self.weight = nn.Parameter(torch.ones(num_features))\n",
    "            self.bias = nn.Parameter(torch.zeros(num_features))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        if self.track_running_stats:\n",
    "            self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "            self.register_buffer('running_var', torch.ones(num_features))\n",
    "        else:\n",
    "            self.register_buffer('running_mean', None)\n",
    "            self.register_buffer('running_var', None)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return NumbaBatchNorm2dFunction.apply(\n",
    "            x, self.weight, self.bias, \n",
    "            self.running_mean, self.running_var, \n",
    "            self.eps, self.momentum, self.training\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def conv2d_kernel(input, kernel, output, padding: int, stride: int):\n",
    "\n",
    "    combined_idx, out_y, out_x = cuda.grid(3)\n",
    "    batch_size, in_channels, in_height, in_width = input.shape\n",
    "    out_channels, _, kernel_height, kernel_width = kernel.shape\n",
    "    out_height, out_width = output.shape[2:]\n",
    "\n",
    "    batch_idx = combined_idx // out_channels\n",
    "    out_channel_idx = combined_idx % out_channels\n",
    "\n",
    "    if batch_idx < batch_size and out_channel_idx < out_channels and out_y < out_height and out_x < out_width:\n",
    "        res = 0.0\n",
    "        for in_channel in range(in_channels):\n",
    "            for ky in range(kernel_height):\n",
    "                for kx in range(kernel_width):\n",
    "                    in_y = out_y * stride - padding + ky\n",
    "                    in_x = out_x * stride - padding + kx\n",
    "                    if 0 <= in_y < in_height and 0 <= in_x < in_width:\n",
    "                        res += input[batch_idx, in_channel, in_y, in_x] * kernel[out_channel_idx, in_channel, ky, kx]\n",
    "        output[batch_idx, out_channel_idx, out_y, out_x] = res\n",
    "\n",
    "\n",
    "class Conv2dFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input: Tensor, weight: Tensor, bias: Optional[Tensor], stride: int, padding: int) -> Tensor:\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "\n",
    "        batch_size, in_channels, in_height, in_width = input.shape\n",
    "        out_channels, _, kernel_height, kernel_width = weight.shape\n",
    "        out_height = (in_height + 2 * padding - kernel_height) // stride + 1\n",
    "        out_width = (in_width + 2 * padding - kernel_width) // stride + 1\n",
    "\n",
    "        output = torch.zeros(batch_size, out_channels, out_height, out_width, device=input.device)\n",
    "\n",
    "        threads_per_block = (8, 8, 8)\n",
    "        blocks_per_grid = (\n",
    "            (batch_size * out_channels + threads_per_block[0] - 1) // threads_per_block[0],\n",
    "            (out_height + threads_per_block[1] - 1) // threads_per_block[1],\n",
    "            (out_width + threads_per_block[2] - 1) // threads_per_block[2]\n",
    "        )\n",
    "\n",
    "        conv2d_kernel[blocks_per_grid, threads_per_block](\n",
    "            input.detach(), weight.detach(), output, padding, stride\n",
    "        )\n",
    "\n",
    "        if bias is not None:\n",
    "            output += bias.view(1, -1, 1, 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor) -> Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor], None, None]:\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        stride = ctx.stride\n",
    "        padding = ctx.padding\n",
    "\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = torch.nn.grad.conv2d_input(input.shape, weight, grad_output, stride, padding)\n",
    "\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = torch.nn.grad.conv2d_weight(input, weight.shape, grad_output, stride, padding)\n",
    "\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum((0, 2, 3))\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None\n",
    "\n",
    "\n",
    "class NumbaConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int,\n",
    "                 kernel_size: int,\n",
    "                 padding=0,\n",
    "                 stride=1,\n",
    "                 bias=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return Conv2dFunction.apply(x, self.weight, self.bias, self.stride, self.padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FLOAT32 = torch.finfo(torch.float32).min\n",
    "\n",
    "@cuda.jit\n",
    "def max_pool_2d_kernel(input, output, kernel_size: int, padding: int, stride: int):\n",
    "    \n",
    "    idx, out_h, out_w = cuda.grid(3)\n",
    "    \n",
    "    batch_idx = idx // input.shape[1]\n",
    "    channel = idx % input.shape[1]\n",
    "    \n",
    "    if batch_idx < input.shape[0] and channel < input.shape[1] and out_h < output.shape[2] and out_w < output.shape[3]:\n",
    "        max_val = MIN_FLOAT32\n",
    "        for ky in range(kernel_size):\n",
    "            for kx in range(kernel_size):\n",
    "                in_y = out_h * stride - padding + ky\n",
    "                in_x = out_w * stride - padding + kx\n",
    "                if 0 <= in_y < input.shape[2] and 0 <= in_x < input.shape[3]:\n",
    "                    max_val = max(max_val, input[batch_idx, channel, in_y, in_x])\n",
    "        output[batch_idx, channel, out_h, out_w] = max_val\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def max_pool_2d_backward_kernel(input, output, grad_output, grad_input, kernel_size: int, padding: int, stride: int):\n",
    "    idx, in_h, in_w = cuda.grid(3)\n",
    "    \n",
    "    batch_idx = idx // input.shape[1]\n",
    "    channel = idx % input.shape[1]\n",
    "    \n",
    "    if batch_idx < input.shape[0] and channel < input.shape[1] and in_h < input.shape[2] and in_w < input.shape[3]:\n",
    "        for ky in range(kernel_size):\n",
    "            for kx in range(kernel_size):\n",
    "                out_h = (in_h + padding - ky) // stride\n",
    "                out_w = (in_w + padding - kx) // stride\n",
    "                if 0 <= out_h < output.shape[2] and 0 <= out_w < output.shape[3]:\n",
    "                    if input[batch_idx, channel, in_h, in_w] == output[batch_idx, channel, out_h, out_w]:\n",
    "                        cuda.atomic.add(grad_input, (batch_idx, channel, in_h, in_w), grad_output[batch_idx, channel, out_h, out_w])\n",
    "\n",
    "\n",
    "class MaxPool2dFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input: Tensor, kernel_size: int, stride: int, padding: int) -> Tensor:\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.kernel_size = kernel_size\n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "\n",
    "        # Detach input for CUDA operations\n",
    "        input_data = input.detach()\n",
    "\n",
    "        batch_size, channels, in_height, in_width = input.shape\n",
    "        out_height = (in_height + 2 * padding - kernel_size) // stride + 1\n",
    "        out_width = (in_width + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "        output = torch.full((batch_size, channels, out_height, out_width), MIN_FLOAT32, device=input.device)\n",
    "\n",
    "        threads_per_block = (8, 8, 8)\n",
    "        blocks_per_grid = (\n",
    "            math.ceil(batch_size * channels / threads_per_block[0]),\n",
    "            math.ceil(out_height / threads_per_block[1]),\n",
    "            math.ceil(out_width / threads_per_block[2])\n",
    "        )\n",
    "\n",
    "        max_pool_2d_kernel[blocks_per_grid, threads_per_block](\n",
    "            input_data, output, kernel_size, padding, stride\n",
    "        )\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor) -> Tuple[Optional[Tensor], None, None, None]:\n",
    "        input, = ctx.saved_tensors\n",
    "        kernel_size = ctx.kernel_size\n",
    "        stride = ctx.stride\n",
    "        padding = ctx.padding\n",
    "\n",
    "        # Detach tensors for CUDA operations\n",
    "        input_data = input.detach()\n",
    "        grad_output_data = grad_output.detach()\n",
    "\n",
    "        grad_input = torch.zeros_like(input)\n",
    "\n",
    "        threads_per_block = (8, 8, 8)\n",
    "        blocks_per_grid = (\n",
    "            math.ceil(input.shape[0] * input.shape[1] / threads_per_block[0]),\n",
    "            math.ceil(input.shape[2] / threads_per_block[1]),\n",
    "            math.ceil(input.shape[3] / threads_per_block[2])\n",
    "        )\n",
    "\n",
    "        output = MaxPool2dFunction.forward(ctx, input, kernel_size, stride, padding)\n",
    "\n",
    "        max_pool_2d_backward_kernel[blocks_per_grid, threads_per_block](\n",
    "            input_data, output, grad_output_data, grad_input, kernel_size, padding, stride\n",
    "        )\n",
    "\n",
    "        return grad_input, None, None, None\n",
    "\n",
    "\n",
    "class NumbaMaxPool2d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 kernel_size: int,\n",
    "                 padding: Optional[int] = 0,\n",
    "                 stride: Optional[int] = 1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride if stride is not None else kernel_size\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return MaxPool2dFunction.apply(x, self.kernel_size, self.stride, self.padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def relu_kernel(input, output, dim: int):\n",
    "\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < dim:\n",
    "        output[idx] = max(input[idx], 0)\n",
    "\n",
    "\n",
    "class NumbaReLUFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input: Tensor) -> Tensor:\n",
    "        output = torch.zeros_like(input)\n",
    "        threads_per_block = 256\n",
    "        dim = input.numel()\n",
    "        blocks_per_grid = math.ceil(dim / threads_per_block)\n",
    "        \n",
    "        relu_kernel[blocks_per_grid, threads_per_block](input.detach().view(-1), output.view(-1), dim)\n",
    "        \n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor) -> Tensor:\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class NumbaReLU(nn.Module):\n",
    "    def __init__(self, inplace: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return NumbaReLUFunction.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPB = 32\n",
    "\n",
    "@cuda.jit\n",
    "def linear_kernel(input, output, weight):\n",
    "  \n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x\n",
    "\n",
    "    tmp = 0.0\n",
    "    for i in range(bpg):\n",
    "        sA[ty, tx] = 0\n",
    "        sB[ty, tx] = 0\n",
    "        if y < input.shape[0] and (tx+i*TPB) < input.shape[1]:\n",
    "            sA[ty, tx] = input[y, tx + i * TPB]\n",
    "        if x < weight.shape[1] and (ty+i*TPB) < weight.shape[0]:\n",
    "            sB[ty, tx] = weight[ty + i * TPB, x]\n",
    "\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        cuda.syncthreads()\n",
    "    if y < output.shape[0] and x < output.shape[1]:\n",
    "        output[y, x] = tmp\n",
    "\n",
    "\n",
    "class NumbaLinearFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        \n",
    "        output = torch.empty(input.size(0), weight.size(0), device=input.device)\n",
    "        \n",
    "        threads_per_block = (TPB, TPB)\n",
    "        grid_y_max = max(input.shape[0], weight.shape[0])\n",
    "        grid_x_max = max(input.shape[1], weight.shape[1])\n",
    "\n",
    "        blocks_per_grid_x = math.ceil(grid_x_max / threads_per_block[0])\n",
    "        blocks_per_grid_y = math.ceil(grid_y_max / threads_per_block[1])\n",
    "\n",
    "        blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "        \n",
    "        linear_kernel[blocks_per_grid, threads_per_block](\n",
    "            input.detach(), output, weight.detach().T\n",
    "        )\n",
    "        \n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor) -> Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor]]:\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "\n",
    "\n",
    "class NumbaLinear(nn.Module):\n",
    " \n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 bias: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return NumbaLinearFunction.apply(x, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet9 Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumbaConvBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel: int = 3,\n",
    "                 stride: int = 1,\n",
    "                 padding: int = 1,\n",
    "                 pooling: bool = False,\n",
    "                 pooling_kernel: int = 4) -> None:\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            NumbaConv2d(in_channels, out_channels, kernel_size=kernel, stride=stride, padding=padding),\n",
    "            NumbaBatchNorm2d(out_channels),\n",
    "            NumbaReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        if pooling:\n",
    "            self.conv.append(NumbaMaxPool2d(kernel_size=pooling_kernel))\n",
    "\n",
    "    def forward(self, X: Tensor):\n",
    "        return self.conv(X)\n",
    "    \n",
    "\n",
    "class NumbaResNet9(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 num_classes: int,) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = NumbaConvBlock(in_channels=in_channels, out_channels=64)\n",
    "        self.conv2 = NumbaConvBlock(in_channels=64, out_channels=128, pooling=True)\n",
    "        \n",
    "        self.residual1 = nn.Sequential(\n",
    "            NumbaConvBlock(128, 128),\n",
    "            NumbaConvBlock(128, 128)\n",
    "        )\n",
    "\n",
    "        self.conv3 = NumbaConvBlock(in_channels=128, out_channels=256, pooling=True)\n",
    "        self.conv4 = NumbaConvBlock(in_channels=256, out_channels=512, pooling=True)\n",
    "        \n",
    "        self.residual2 = nn.Sequential(\n",
    "            NumbaConvBlock(512, 512),\n",
    "            NumbaConvBlock(512, 512)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            NumbaMaxPool2d(4),\n",
    "            nn.Flatten(),\n",
    "            NumbaLinear(in_features=512, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.residual1(x) + x\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.residual2(x) + x\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model = NumbaResNet9(in_channels=3, num_classes=10).to(device)\n",
    "\n",
    "\n",
    "input_tensor = torch.randn(100, 3, 32, 32).to(device) \n",
    "\n",
    "output = model(input_tensor) # warm up\n",
    "torch.cuda.synchronize() # Wait for all kernels in all streams on a CUDA device to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    output = model(input_tensor)\n",
    "run_time = time.time() - start\n",
    "\n",
    "print(f\"Numba Resnet9 Time: {run_time:.4f} secs \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

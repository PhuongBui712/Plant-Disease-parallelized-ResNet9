{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Function\n",
    "import math\n",
    "from numba import cuda\n",
    "from typing import Optional, Tuple\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print (torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def batchnorm2d_forward_kernel(input, output, mean, inv_std, gamma, beta):\n",
    "    idx, out_h, out_w = cuda.grid(3)\n",
    "\n",
    "    batch_idx = idx // input.shape[1]\n",
    "    channel = idx % input.shape[1]\n",
    "\n",
    "    if batch_idx < output.shape[0] and channel < output.shape[1] and out_h < output.shape[2] and out_w < output.shape[3]:\n",
    "        normalized = (input[batch_idx, channel, out_h, out_w] - mean[channel]) * inv_std[channel]\n",
    "        output[batch_idx, channel, out_h, out_w] = normalized * gamma[channel] + beta[channel]\n",
    "\n",
    "\n",
    "class NumbaBatchNorm2dFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,\n",
    "                input: Tensor,\n",
    "                gamma: Tensor, \n",
    "                beta: Tensor, \n",
    "                running_mean: Optional[Tensor], \n",
    "                running_var: Optional[Tensor], \n",
    "                eps: float, \n",
    "                momentum: float, \n",
    "                training: bool) -> Tensor:\n",
    "        input = input.contiguous()\n",
    "        \n",
    "        if training:\n",
    "            mean = input.mean(dim=(0, 2, 3))\n",
    "            var = input.var(dim=(0, 2, 3), unbiased=False)\n",
    "            \n",
    "            if running_mean is not None:\n",
    "                running_mean.mul_(1 - momentum).add_(mean * momentum)\n",
    "            if running_var is not None:\n",
    "                running_var.mul_(1 - momentum).add_(var * momentum)\n",
    "        else:\n",
    "            mean = running_mean\n",
    "            var = running_var\n",
    "        \n",
    "        inv_std = 1 / torch.sqrt(var + eps)\n",
    "        output = torch.empty_like(input)\n",
    "        \n",
    "        threads_per_block = (8, 8, 8)\n",
    "        blocks_per_grid = (\n",
    "            math.ceil(input.shape[0] * input.shape[1] / threads_per_block[0]),\n",
    "            math.ceil(input.shape[2] / threads_per_block[1]),\n",
    "            math.ceil(input.shape[3] / threads_per_block[2])\n",
    "        )\n",
    "\n",
    "        batchnorm2d_forward_kernel[blocks_per_grid, threads_per_block](\n",
    "            input.detach(), output, mean.detach(), inv_std.detach(), gamma.detach(), beta.detach()\n",
    "        )\n",
    "        \n",
    "        ctx.save_for_backward(input, gamma, mean, inv_std)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor) -> Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor], None, None, None, None, None]:\n",
    "        input, gamma, mean, inv_std = ctx.saved_tensors\n",
    "        \n",
    "        # Use PyTorch's built-in backward pass for simplicity and correctness\n",
    "        normalized = (input - mean[None, :, None, None]) * inv_std[None, :, None, None]\n",
    "        grad_input = F.batch_norm(\n",
    "            input, mean, 1/inv_std**2, gamma, None, \n",
    "            eps=0, momentum=0, training=True\n",
    "        )\n",
    "        grad_input = grad_output * grad_input\n",
    "        \n",
    "        grad_gamma = (grad_output * normalized).sum(dim=(0, 2, 3))\n",
    "        grad_beta = grad_output.sum(dim=(0, 2, 3))\n",
    "        \n",
    "        return grad_input, grad_gamma, grad_beta, None, None, None, None, None\n",
    "\n",
    "\n",
    "class NumbaBatchNorm2d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 eps: float = 1e-05,\n",
    "                 momentum: float = 0.1,\n",
    "                 affine: bool = True,\n",
    "                 track_running_stats: bool = True) -> None:\n",
    "                 \n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        \n",
    "        if self.affine:\n",
    "            self.weight = nn.Parameter(torch.ones(num_features))\n",
    "            self.bias = nn.Parameter(torch.zeros(num_features))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        if self.track_running_stats:\n",
    "            self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "            self.register_buffer('running_var', torch.ones(num_features))\n",
    "        else:\n",
    "            self.register_buffer('running_mean', None)\n",
    "            self.register_buffer('running_var', None)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return NumbaBatchNorm2dFunction.apply(\n",
    "            x, self.weight, self.bias, \n",
    "            self.running_mean, self.running_var, \n",
    "            self.eps, self.momentum, self.training\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    batch_size = 1000\n",
    "    num_features = 3  \n",
    "    height = 512 \n",
    "    width = 512\n",
    "    \n",
    "\n",
    "    input_tensor1 = torch.randn(batch_size, num_features, height, width, device='cuda')\n",
    "    input_tensor2 = torch.randn(batch_size, num_features, height, width, device = 'cuda')\n",
    "\n",
    "    #custom batchnom2d\n",
    "    start_time = time.time()\n",
    "    numba_batch_norm = NumbaBatchNorm2d(num_features).cuda()\n",
    "    for _ in range(100):\n",
    "        _ = numba_batch_norm(input_tensor1)\n",
    "    custom_time = time.time() - start_time\n",
    "    \n",
    "    #pytorch built-in batchnom2d\n",
    "    start_time = time.time()\n",
    "    pytorch_batch_norm = nn.BatchNorm2d(num_features).cuda()\n",
    "    for _ in range(100):\n",
    "        _ = pytorch_batch_norm(input_tensor2)\n",
    "    pytorch_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Custom BatchNorm2d Time: {custom_time:.4f} secs \")\n",
    "    print(f\"PyTorch BatchNorm2d Time: {pytorch_time:.4f} secs \")\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

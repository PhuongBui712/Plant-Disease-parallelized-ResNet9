{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from numba import cuda, float32\n",
    "from typing import Optional, Tuple\n",
    "import time\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.autograd import Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPB = 32\n",
    "\n",
    "@cuda.jit\n",
    "def linear_kernel(input, output, weight):\n",
    "  \n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x\n",
    "\n",
    "    tmp = 0.0\n",
    "    for i in range(bpg):\n",
    "        sA[ty, tx] = 0\n",
    "        sB[ty, tx] = 0\n",
    "        if y < input.shape[0] and (tx+i*TPB) < input.shape[1]:\n",
    "            sA[ty, tx] = input[y, tx + i * TPB]\n",
    "        if x < weight.shape[1] and (ty+i*TPB) < weight.shape[0]:\n",
    "            sB[ty, tx] = weight[ty + i * TPB, x]\n",
    "\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        cuda.syncthreads()\n",
    "    if y < output.shape[0] and x < output.shape[1]:\n",
    "        output[y, x] = tmp\n",
    "\n",
    "\n",
    "class NumbaLinearFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        \n",
    "        output = torch.empty(input.size(0), weight.size(0), device=input.device)\n",
    "        \n",
    "        threads_per_block = (TPB, TPB)\n",
    "        grid_y_max = max(input.shape[0], weight.shape[0])\n",
    "        grid_x_max = max(input.shape[1], weight.shape[1])\n",
    "\n",
    "        blocks_per_grid_x = math.ceil(grid_x_max / threads_per_block[0])\n",
    "        blocks_per_grid_y = math.ceil(grid_y_max / threads_per_block[1])\n",
    "\n",
    "        blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "        \n",
    "        linear_kernel[blocks_per_grid, threads_per_block](\n",
    "            input.detach(), output, weight.detach().T\n",
    "        )\n",
    "        \n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor) -> Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor]]:\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "\n",
    "\n",
    "class NumbaLinear(nn.Module):\n",
    " \n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 bias: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return NumbaLinearFunction.apply(x, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_features = 1024\n",
    "out_features = 512\n",
    "batch_size = 1000\n",
    "\n",
    "\n",
    "input_tensor1 = torch.randn(batch_size, in_features).cuda()  \n",
    "input_tensor2 = torch.randn(batch_size, in_features, device=\"cpu\")\n",
    "\n",
    "numba_linear = NumbaLinear(in_features, out_features).cuda() \n",
    "pytorch_linear = nn.Linear(in_features, out_features)  \n",
    "\n",
    "_ = numba_linear(input_tensor1)\n",
    "_ = pytorch_linear (input_tensor2)\n",
    "torch.cuda.synchronize() \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch built-in Linear execution time: 1.660566 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for _ in range (1000):\n",
    "    output_pytorch = pytorch_linear(input_tensor2)\n",
    "pytorch_time = time.time() - start_time\n",
    "print(f\"PyTorch built-in Linear execution time: {pytorch_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom NumbaLinear execution time: 0.397803 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for _ in range (1009):\n",
    "    output_numba = numba_linear(input_tensor1)\n",
    "numba_time = time.time() - start_time\n",
    "print(f\"Custom NumbaLinear execution time: {numba_time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

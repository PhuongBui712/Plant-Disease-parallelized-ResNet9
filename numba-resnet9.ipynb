{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b61ae19",
   "metadata": {
    "papermill": {
     "duration": 0.009609,
     "end_time": "2024-08-11T05:56:05.315145",
     "exception": false,
     "start_time": "2024-08-11T05:56:05.305536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed9e907",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:05.334420Z",
     "iopub.status.busy": "2024-08-11T05:56:05.334137Z",
     "iopub.status.idle": "2024-08-11T05:56:11.160306Z",
     "shell.execute_reply": "2024-08-11T05:56:11.159529Z"
    },
    "papermill": {
     "duration": 5.838448,
     "end_time": "2024-08-11T05:56:11.162676",
     "exception": false,
     "start_time": "2024-08-11T05:56:05.324228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from numba import cuda, float32, float64\n",
    "from typing import Optional, Callable, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Function\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed948165",
   "metadata": {},
   "source": [
    "We set the default `dtype` of `torch` is `float32` to optimize speed when working on cuda of `numba`. We also set the default device to process is `cuda:0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba00c87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:11.182695Z",
     "iopub.status.busy": "2024-08-11T05:56:11.182260Z",
     "iopub.status.idle": "2024-08-11T05:56:11.186439Z",
     "shell.execute_reply": "2024-08-11T05:56:11.185663Z"
    },
    "papermill": {
     "duration": 0.016302,
     "end_time": "2024-08-11T05:56:11.188442",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.172140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9a879",
   "metadata": {
    "papermill": {
     "duration": 0.009285,
     "end_time": "2024-08-11T05:56:11.206796",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.197511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Implement ResNet9 with Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98562d55",
   "metadata": {},
   "source": [
    "We will implement all the essential modules by integrating `pytorch` with `numba` to leverage the autograde power of `pytorch` and parallelize the forward pass of these by `numba` cuda.\n",
    "\n",
    "We need to implement these following modules: `Conv2d`, `MaxPool2d`, `BatchNorm2d`, `ReLU`, and `Linear`. These modules will have same prototype like following image:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./assets/pytorch-and-numba.png\" alt=\"Description of Image\" width=\"750\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "As shown in the image, we will build these module as we often do with normal customized `torch` module, but instead of using `torch` operations and functions in `forward` pass, we just calculate the number of **thread per block** and **block per grid**. Then we will call a outside `numba` **cuda kernel** to perform all operations of that module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0b719",
   "metadata": {
    "papermill": {
     "duration": 0.008906,
     "end_time": "2024-08-11T05:56:11.224706",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.215800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef4127",
   "metadata": {},
   "source": [
    "This section implements a custom 2D convolution layer using Numba CUDA for efficient GPU acceleration. It consists of three parts:\n",
    "\n",
    "1. **`conv2d_kernel`:** This is the Numba CUDA kernel that performs the actual convolution operation. It takes the input tensor, convolution kernel, output tensor, padding, and stride as arguments. The kernel iterates over each output element and calculates the weighted sum of the corresponding input elements using the kernel.\n",
    "\n",
    "2. **`Conv2dFunction`:** This class defines the forward and backward passes of the convolution operation. The `forward` method calculates the output tensor by calling the `conv2d_kernel` and applies the bias if provided. The `backward` method calculates the gradients for the input, weight, and bias using PyTorch's autograd functionality.\n",
    "\n",
    "3. **`NumbaConv2d`:** This class inherits from `nn.Module` and defines the convolution layer as a PyTorch module. It initializes the weight and bias parameters and calls the `Conv2dFunction` in its `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e90f884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:11.244256Z",
     "iopub.status.busy": "2024-08-11T05:56:11.243993Z",
     "iopub.status.idle": "2024-08-11T05:56:11.264837Z",
     "shell.execute_reply": "2024-08-11T05:56:11.264105Z"
    },
    "papermill": {
     "duration": 0.032939,
     "end_time": "2024-08-11T05:56:11.266688",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.233749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def conv2d_kernel(input, kernel, output, padding: int, stride: int):\n",
    "    \"\"\"\n",
    "    Performs a 2D convolution operation on a 4D tensor.\n",
    "\n",
    "    Args:\n",
    "        input: The input tensor.\n",
    "        kernel: The convolution kernel.\n",
    "        output: The output tensor.\n",
    "        padding (int): The amount of padding to apply.\n",
    "        stride (int): The stride of the convolution operation.\n",
    "    \"\"\"\n",
    "    combined_idx, out_y, out_x = cuda.grid(3)\n",
    "    batch_size, in_channels, in_height, in_width = input.shape\n",
    "    out_channels, _, kernel_height, kernel_width = kernel.shape\n",
    "    out_height, out_width = output.shape[2:]\n",
    "\n",
    "    batch_idx = combined_idx // out_channels\n",
    "    out_channel_idx = combined_idx % out_channels\n",
    "\n",
    "    if batch_idx < batch_size and out_channel_idx < out_channels and out_y < out_height and out_x < out_width:\n",
    "        res = 0.0\n",
    "        for in_channel in range(in_channels):\n",
    "            for ky in range(kernel_height):\n",
    "                for kx in range(kernel_width):\n",
    "                    in_y = out_y * stride - padding + ky\n",
    "                    in_x = out_x * stride - padding + kx\n",
    "                    if 0 <= in_y < in_height and 0 <= in_x < in_width:\n",
    "                        res += input[batch_idx, in_channel, in_y, in_x] * kernel[out_channel_idx, in_channel, ky, kx]\n",
    "        output[batch_idx, out_channel_idx, out_y, out_x] = res\n",
    "\n",
    "class Conv2dFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias, stride, padding):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "\n",
    "        batch_size, in_channels, in_height, in_width = input.shape\n",
    "        out_channels, _, kernel_height, kernel_width = weight.shape\n",
    "        out_height = (in_height + 2 * padding - kernel_height) // stride + 1\n",
    "        out_width = (in_width + 2 * padding - kernel_width) // stride + 1\n",
    "\n",
    "        output = torch.zeros(batch_size, out_channels, out_height, out_width, device=input.device)\n",
    "\n",
    "        threads_per_block = (8, 8, 8)\n",
    "        blocks_per_grid = (\n",
    "            (batch_size * out_channels + threads_per_block[0] - 1) // threads_per_block[0],\n",
    "            (out_height + threads_per_block[1] - 1) // threads_per_block[1],\n",
    "            (out_width + threads_per_block[2] - 1) // threads_per_block[2]\n",
    "        )\n",
    "\n",
    "        conv2d_kernel[blocks_per_grid, threads_per_block](\n",
    "            input.detach(), weight.detach(), output, padding, stride\n",
    "        )\n",
    "\n",
    "        if bias is not None:\n",
    "            output += bias.view(1, -1, 1, 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        stride = ctx.stride\n",
    "        padding = ctx.padding\n",
    "\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = torch.nn.grad.conv2d_input(input.shape, weight, grad_output, stride, padding)\n",
    "\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = torch.nn.grad.conv2d_weight(input, weight.shape, grad_output, stride, padding)\n",
    "\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum((0, 2, 3))\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None\n",
    "\n",
    "class NumbaConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Performs a 2D convolution operation on a 4D tensor using Numba CUDA.\n",
    "\n",
    "    This class implements a convolution operation with configurable input and output channels, kernel size, padding, and stride.\n",
    "    It leverages Numba CUDA for efficient GPU acceleration.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): The number of input channels.\n",
    "        out_channels (int): The number of output channels.\n",
    "        kernel_size (int): The size of the convolution kernel.\n",
    "        padding (Optional[int], optional): The amount of padding to apply. Defaults to 0.\n",
    "        stride (Optional[int], optional): The stride of the convolution operation. Defaults to 1.\n",
    "        weight (Optional[torch.Tensor], optional): The initial weight tensor. Defaults to None.\n",
    "        bias (Optional[torch.Tensor], optional): The initial bias tensor. Defaults to None.\n",
    "\n",
    "    Example:\n",
    "        >>> conv = NumbaConv2D(in_channels=3, out_channels=64, kernel_size=3, padding=1, stride=2)\n",
    "        >>> input_tensor = torch.randn(16, 3, 512, 512, device='cuda')\n",
    "        >>> output_tensor = conv(input_tensor)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int,\n",
    "                 kernel_size: int,\n",
    "                 padding=0,\n",
    "                 stride=1,\n",
    "                 bias=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return Conv2dFunction.apply(x, self.weight, self.bias, self.stride, self.padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc357503",
   "metadata": {
    "papermill": {
     "duration": 0.009728,
     "end_time": "2024-08-11T05:56:11.285543",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.275815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### MaxPool2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e3c6fb",
   "metadata": {},
   "source": [
    "This section implements a custom 2D max pooling layer using Numba CUDA for efficient GPU acceleration. It consists of three parts:\n",
    "\n",
    "1. **`max_pool_2d_kernel`:** This is the Numba CUDA kernel that performs the actual max pooling operation. It takes the input tensor, kernel size, padding, and stride as arguments. The kernel iterates over each output element and calculates the maximum value within the corresponding kernel window.\n",
    "\n",
    "2. **`MaxPool2dFunction`:** This class defines the forward and backward passes of the max pooling operation. The `forward` method calculates the output tensor by calling the `max_pool_2d_kernel` and applies the bias if provided. The `backward` method calculates the gradients for the input using PyTorch's autograd functionality.\n",
    "\n",
    "3. **`NumbaMaxPool2d`:** This class inherits from `nn.Module` and defines the max pooling layer as a PyTorch module. It initializes the kernel size, padding, and stride parameters and calls the `MaxPool2dFunction` in its `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "353932bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:11.305120Z",
     "iopub.status.busy": "2024-08-11T05:56:11.304869Z",
     "iopub.status.idle": "2024-08-11T05:56:11.327705Z",
     "shell.execute_reply": "2024-08-11T05:56:11.326943Z"
    },
    "papermill": {
     "duration": 0.034871,
     "end_time": "2024-08-11T05:56:11.329564",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.294693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MIN_FLOAT32 = torch.finfo(torch.float32).min\n",
    "\n",
    "@cuda.jit\n",
    "def max_pool_2d_kernel(input, output, kernel_size: int, padding: int, stride: int):\n",
    "    \"\"\"\n",
    "    Performs a 2D max pooling operation on a 4D tensor.\n",
    "\n",
    "    Args:\n",
    "        input: The input tensor.\n",
    "        output: The output tensor.\n",
    "        kernel_size (int): The size of the pooling kernel.\n",
    "        padding (int): The amount of padding to apply.\n",
    "        stride (int): The stride of the pooling operation.\n",
    "    \"\"\"\n",
    "    idx, out_h, out_w = cuda.grid(3)\n",
    "    \n",
    "    batch_idx = idx // input.shape[1]\n",
    "    channel = idx % input.shape[1]\n",
    "    \n",
    "    if batch_idx < input.shape[0] and channel < input.shape[1] and out_h < output.shape[2] and out_w < output.shape[3]:\n",
    "        max_val = MIN_FLOAT32\n",
    "        for ky in range(kernel_size):\n",
    "            for kx in range(kernel_size):\n",
    "                in_y = out_h * stride - padding + ky\n",
    "                in_x = out_w * stride - padding + kx\n",
    "                if 0 <= in_y < input.shape[2] and 0 <= in_x < input.shape[3]:\n",
    "                    max_val = max(max_val, input[batch_idx, channel, in_y, in_x])\n",
    "        output[batch_idx, channel, out_h, out_w] = max_val\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def max_pool_2d_backward_kernel(input, output, grad_output, grad_input, kernel_size, padding, stride):\n",
    "    \"\"\"\n",
    "    Performs the backward pass for a 2D max pooling operation on a 4D tensor.\n",
    "\n",
    "    This kernel calculates the gradient of the input tensor based on the gradient of the output tensor and the\n",
    "    pooling operation's parameters. It uses atomic addition to accumulate gradients for elements that contributed\n",
    "    to the maximum value in the pooling window.\n",
    "\n",
    "    Args:\n",
    "        input: The input tensor.\n",
    "        output: The output tensor.\n",
    "        grad_output: The gradient of the output tensor.\n",
    "        grad_input: The gradient of the input tensor (to be accumulated).\n",
    "        kernel_size (int): The size of the pooling kernel.\n",
    "        padding (int): The amount of padding applied during the forward pass.\n",
    "        stride (int): The stride of the pooling operation.\n",
    "    \"\"\"\n",
    "    idx, in_h, in_w = cuda.grid(3)\n",
    "    \n",
    "    batch_idx = idx // input.shape[1]\n",
    "    channel = idx % input.shape[1]\n",
    "    \n",
    "    if batch_idx < input.shape[0] and channel < input.shape[1] and in_h < input.shape[2] and in_w < input.shape[3]:\n",
    "        for ky in range(kernel_size):\n",
    "            for kx in range(kernel_size):\n",
    "                out_h = (in_h + padding - ky) // stride\n",
    "                out_w = (in_w + padding - kx) // stride\n",
    "                if 0 <= out_h < output.shape[2] and 0 <= out_w < output.shape[3]:\n",
    "                    if input[batch_idx, channel, in_h, in_w] == output[batch_idx, channel, out_h, out_w]:\n",
    "                        cuda.atomic.add(grad_input, (batch_idx, channel, in_h, in_w), grad_output[batch_idx, channel, out_h, out_w])\n",
    "\n",
    "\n",
    "class MaxPool2dFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, kernel_size, stride, padding):\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.kernel_size = kernel_size\n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "\n",
    "        # Detach input for CUDA operations\n",
    "        input_data = input.detach()\n",
    "\n",
    "        batch_size, channels, in_height, in_width = input.shape\n",
    "        out_height = (in_height + 2 * padding - kernel_size) // stride + 1\n",
    "        out_width = (in_width + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "        output = torch.full((batch_size, channels, out_height, out_width), MIN_FLOAT32, device=input.device)\n",
    "\n",
    "        threads_per_block = (8, 8, 8)\n",
    "        blocks_per_grid = (\n",
    "            math.ceil(batch_size * channels / threads_per_block[0]),\n",
    "            math.ceil(out_height / threads_per_block[1]),\n",
    "            math.ceil(out_width / threads_per_block[2])\n",
    "        )\n",
    "\n",
    "        max_pool_2d_kernel[blocks_per_grid, threads_per_block](\n",
    "            input_data, output, kernel_size, padding, stride\n",
    "        )\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        kernel_size = ctx.kernel_size\n",
    "        stride = ctx.stride\n",
    "        padding = ctx.padding\n",
    "\n",
    "        # Detach tensors for CUDA operations\n",
    "        input_data = input.detach()\n",
    "        grad_output_data = grad_output.detach()\n",
    "\n",
    "        grad_input = torch.zeros_like(input)\n",
    "\n",
    "        threads_per_block = (8, 8, 8)\n",
    "        blocks_per_grid = (\n",
    "            math.ceil(input.shape[0] * input.shape[1] / threads_per_block[0]),\n",
    "            math.ceil(input.shape[2] / threads_per_block[1]),\n",
    "            math.ceil(input.shape[3] / threads_per_block[2])\n",
    "        )\n",
    "\n",
    "        output = MaxPool2dFunction.forward(ctx, input, kernel_size, stride, padding)\n",
    "\n",
    "        max_pool_2d_backward_kernel[blocks_per_grid, threads_per_block](\n",
    "            input_data, output, grad_output_data, grad_input, kernel_size, padding, stride\n",
    "        )\n",
    "\n",
    "        return grad_input, None, None, None\n",
    "\n",
    "\n",
    "class NumbaMaxPool2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Performs a 2D max pooling operation on a 4D tensor using Numba CUDA.\n",
    "\n",
    "    This class implements a max pooling operation with configurable kernel size, padding, and stride.\n",
    "    It leverages Numba CUDA for efficient GPU acceleration.\n",
    "\n",
    "    Args:\n",
    "        kernel_size (int): The size of the pooling kernel.\n",
    "        padding (Optional[int], optional): The amount of padding to apply. Defaults to 0.\n",
    "        stride (Optional[int], optional): The stride of the pooling operation. Defaults to 1.\n",
    "\n",
    "    Example:\n",
    "        >>> pool = NumbaMaxPool2d(kernel_size=2, padding=1, stride=2)\n",
    "        >>> input_tensor = torch.randn(16, 3, 512, 512, device='cuda')\n",
    "        >>> output_tensor = pool(input_tensor)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 kernel_size: int,\n",
    "                 padding: Optional[int] = 0,\n",
    "                 stride: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride if stride is not None else kernel_size\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        return MaxPool2dFunction.apply(x, self.kernel_size, self.stride, self.padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20abb5",
   "metadata": {
    "papermill": {
     "duration": 0.008952,
     "end_time": "2024-08-11T05:56:11.347825",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.338873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### BatchNorm2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65e4ed",
   "metadata": {},
   "source": [
    "This section implements a custom Batch Normalization layer using Numba CUDA for efficient GPU acceleration. It consists of three parts:\n",
    "\n",
    "1. **`batchnorm2d_forward_kernel`:** This is the Numba CUDA kernel that performs the actual batch normalization operation. It takes the input tensor, output tensor, mean, inverse standard deviation, scaling factor (gamma), and shifting factor (beta) as arguments. The kernel iterates over each element in the input tensor and applies the batch normalization formula to calculate the corresponding output element.\n",
    "\n",
    "2. **`NumbaBatchNorm2dFunction`:** This class defines the forward and backward passes of the batch normalization operation. The `forward` method calculates the output tensor by calling the `batchnorm2d_forward_kernel` and applies the scaling and shifting factors if provided. It also updates the running mean and variance if the layer is in training mode. The `backward` method calculates the gradients for the input, scaling factor, and shifting factor using PyTorch's autograd functionality.\n",
    "\n",
    "3. **`NumbaBatchNorm2d`:** This class inherits from `nn.Module` and defines the batch normalization layer as a PyTorch module. It initializes the scaling and shifting parameters and calls the `NumbaBatchNorm2dFunction` in its `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ed97c4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:11.367416Z",
     "iopub.status.busy": "2024-08-11T05:56:11.367142Z",
     "iopub.status.idle": "2024-08-11T05:56:11.388761Z",
     "shell.execute_reply": "2024-08-11T05:56:11.387952Z"
    },
    "papermill": {
     "duration": 0.033604,
     "end_time": "2024-08-11T05:56:11.390591",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.356987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def batchnorm2d_forward_kernel(input, output, mean, inv_std, gamma, beta):\n",
    "    \"\"\"\n",
    "    A CUDA kernel that performs batch normalization on a 4D tensor.\n",
    "\n",
    "    Args:\n",
    "        input: The input tensor.\n",
    "        output: The output tensor.\n",
    "        mean: The mean of the input tensor.\n",
    "        var: The variance of the input tensor.\n",
    "        eps (float): A small value added to the denominator for numerical stability.\n",
    "        gamma: The scaling factor.\n",
    "        beta: The shifting factor.\n",
    "    \"\"\"\n",
    "    idx, out_h, out_w = cuda.grid(3)\n",
    "\n",
    "    batch_idx = idx // input.shape[1]\n",
    "    channel = idx % input.shape[1]\n",
    "\n",
    "    if batch_idx < output.shape[0] and channel < output.shape[1] and out_h < output.shape[2] and out_w < output.shape[3]:\n",
    "        normalized = (input[batch_idx, channel, out_h, out_w] - mean[channel]) * inv_std[channel]\n",
    "        output[batch_idx, channel, out_h, out_w] = normalized * gamma[channel] + beta[channel]\n",
    "\n",
    "\n",
    "class NumbaBatchNorm2dFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, gamma, beta, running_mean, running_var, eps, momentum, training):\n",
    "        input = input.contiguous()\n",
    "        \n",
    "        if training:\n",
    "            mean = input.mean(dim=(0, 2, 3))\n",
    "            var = input.var(dim=(0, 2, 3), unbiased=False)\n",
    "            \n",
    "            if running_mean is not None:\n",
    "                running_mean.mul_(1 - momentum).add_(mean * momentum)\n",
    "            if running_var is not None:\n",
    "                running_var.mul_(1 - momentum).add_(var * momentum)\n",
    "        else:\n",
    "            mean = running_mean\n",
    "            var = running_var\n",
    "        \n",
    "        inv_std = 1 / torch.sqrt(var + eps)\n",
    "        output = torch.empty_like(input)\n",
    "        \n",
    "        threads_per_block = (8, 8, 8)\n",
    "        blocks_per_grid = (\n",
    "            math.ceil(input.shape[0] * input.shape[1] / threads_per_block[0]),\n",
    "            math.ceil(input.shape[2] / threads_per_block[1]),\n",
    "            math.ceil(input.shape[3] / threads_per_block[2])\n",
    "        )\n",
    "\n",
    "        batchnorm2d_forward_kernel[blocks_per_grid, threads_per_block](\n",
    "            input.detach(), output, mean.detach(), inv_std.detach(), gamma.detach(), beta.detach()\n",
    "        )\n",
    "        \n",
    "        ctx.save_for_backward(input, gamma, mean, inv_std)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, gamma, mean, inv_std = ctx.saved_tensors\n",
    "        \n",
    "        # Use PyTorch's built-in backward pass for simplicity and correctness\n",
    "        normalized = (input - mean[None, :, None, None]) * inv_std[None, :, None, None]\n",
    "        grad_input = F.batch_norm(\n",
    "            input, mean, 1/inv_std**2, gamma, None, \n",
    "            eps=0, momentum=0, training=True\n",
    "        )\n",
    "        grad_input = grad_output * grad_input\n",
    "        \n",
    "        grad_gamma = (grad_output * normalized).sum(dim=(0, 2, 3))\n",
    "        grad_beta = grad_output.sum(dim=(0, 2, 3))\n",
    "        \n",
    "        return grad_input, grad_gamma, grad_beta, None, None, None, None, None\n",
    "\n",
    "\n",
    "class NumbaBatchNorm2d(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module that implements a batch normalization layer using Numba for acceleration.\n",
    "\n",
    "    This class is similar to `torch.nn.BatchNorm2d` but uses Numba to perform the mean and variance\n",
    "    calculations on the GPU, potentially leading to faster execution.\n",
    "\n",
    "    Args:\n",
    "        num_features (int): The number of features in the input tensor.\n",
    "        eps (float, optional): A small value added to the denominator for numerical stability.\n",
    "            Defaults to 1e-05.\n",
    "        momentum (float, optional): The momentum used for running mean and variance computation.\n",
    "            Defaults to 0.1.\n",
    "        affine (bool, optional): If True, the layer will learn affine parameters (gamma and beta).\n",
    "            Defaults to True.\n",
    "        track_running_stats (bool, optional): If True, the layer will track running mean and variance.\n",
    "            Defaults to True.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 eps: float = 1e-05,\n",
    "                 momentum: float = 0.1,\n",
    "                 affine: bool = True,\n",
    "                 track_running_stats: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        \n",
    "        if self.affine:\n",
    "            self.weight = nn.Parameter(torch.ones(num_features))\n",
    "            self.bias = nn.Parameter(torch.zeros(num_features))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        if self.track_running_stats:\n",
    "            self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "            self.register_buffer('running_var', torch.ones(num_features))\n",
    "        else:\n",
    "            self.register_buffer('running_mean', None)\n",
    "            self.register_buffer('running_var', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return NumbaBatchNorm2dFunction.apply(\n",
    "            x, self.weight, self.bias, \n",
    "            self.running_mean, self.running_var, \n",
    "            self.eps, self.momentum, self.training\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ef5418",
   "metadata": {
    "papermill": {
     "duration": 0.009312,
     "end_time": "2024-08-11T05:56:11.409264",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.399952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c137500e",
   "metadata": {},
   "source": [
    "This section implements a custom ReLU activation layer using Numba CUDA for efficient GPU acceleration. It consists of three parts:\n",
    "\n",
    "1. **`relu_kernel`:** This is the Numba CUDA kernel that performs the actual ReLU operation. It takes the input tensor, output tensor, and the total number of elements in the input and output arrays as arguments. The kernel iterates over each element in the input tensor and applies the ReLU formula to calculate the corresponding output element.\n",
    "\n",
    "2. **`NumbaReLUFunction`:** This class defines the forward and backward passes of the ReLU operation. The `forward` method calculates the output tensor by calling the `relu_kernel` and applies the ReLU formula to each element. It also saves the input tensor for use in the backward pass. The `backward` method calculates the gradients for the input using PyTorch's autograd functionality.\n",
    "\n",
    "3. **`NumbaReLU`:** This class inherits from `nn.Module` and defines the ReLU layer as a PyTorch module. It calls the `NumbaReLUFunction` in its `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a4296c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:11.429680Z",
     "iopub.status.busy": "2024-08-11T05:56:11.429388Z",
     "iopub.status.idle": "2024-08-11T05:56:11.439260Z",
     "shell.execute_reply": "2024-08-11T05:56:11.438402Z"
    },
    "papermill": {
     "duration": 0.022346,
     "end_time": "2024-08-11T05:56:11.441287",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.418941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def relu_kernel(input, output, dim: int):\n",
    "    \"\"\"\n",
    "    Applies ReLU activation to a CUDA array.\n",
    "\n",
    "    Args:\n",
    "        input: The input CUDA array.\n",
    "        output: The output CUDA array.\n",
    "        dim (int): The total number of elements in the input and output arrays.\n",
    "    \"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < dim:\n",
    "        output[idx] = max(input[idx], 0)\n",
    "\n",
    "\n",
    "class NumbaReLUFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = torch.zeros_like(input)\n",
    "        threads_per_block = 256\n",
    "        dim = input.numel()\n",
    "        blocks_per_grid = math.ceil(dim / threads_per_block)\n",
    "        \n",
    "        relu_kernel[blocks_per_grid, threads_per_block](input.detach().view(-1), output.view(-1), dim)\n",
    "        \n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class NumbaReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies the ReLU function to a CUDA tensor using Numba.\n",
    "\n",
    "    Args:\n",
    "        inplace (bool, optional): If set to `True`, the operation will be performed in-place. Defaults to `False`.\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *)` where `*` means, any number of additional dimensions\n",
    "        - Output: :math:`(N, *)`, same shape as the input\n",
    "\n",
    "    Examples:\n",
    "        >>> m = NumbaReLU()\n",
    "        >>> input = torch.randn(2, 3, 4, 5, device='cuda')\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "    def __init__(self, inplace: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return NumbaReLUFunction.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd06f2",
   "metadata": {
    "papermill": {
     "duration": 0.009575,
     "end_time": "2024-08-11T05:56:11.460750",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.451175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ce561",
   "metadata": {},
   "source": [
    "This section implements a custom linear layer using Numba CUDA for efficient GPU acceleration. It consists of three parts:\n",
    "\n",
    "1. **`linear_kernel`:** This is the Numba CUDA kernel that performs the actual linear operation. It takes the input matrix, output matrix, and the weight matrix as arguments. The kernel iterates over each element in the input tensor and applies the linear formula to calculate the corresponding output element. The kernel uses shared memory to store the input and weight matrices, which allows for faster access to the data.\n",
    "\n",
    "2. **`NumbaLinearFunction`:** This class defines the forward and backward passes of the linear operation. The `forward` method calculates the output tensor by calling the `linear_kernel` and applies the linear formula to each element. It also saves the input tensor, weight tensor, and bias tensor for use in the backward pass. The `backward` method calculates the gradients for the input, weight, and bias using PyTorch's autograd functionality.\n",
    "\n",
    "3. **`NumbaLinear`:** This class inherits from `nn.Module` and defines the linear layer as a PyTorch module. It calls the `NumbaLinearFunction` in its `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8cc3842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:11.481821Z",
     "iopub.status.busy": "2024-08-11T05:56:11.481547Z",
     "iopub.status.idle": "2024-08-11T05:56:11.503332Z",
     "shell.execute_reply": "2024-08-11T05:56:11.502481Z"
    },
    "papermill": {
     "duration": 0.034718,
     "end_time": "2024-08-11T05:56:11.505297",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.470579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TPB = 32\n",
    "\n",
    "@cuda.jit\n",
    "def linear_kernel(input, output, weight):\n",
    "    \"\"\"\n",
    "    Performs a matrix multiplication between an input matrix and a weight matrix using shared memory.\n",
    "\n",
    "    Args:\n",
    "        input: The input matrix.\n",
    "        output: The output matrix.\n",
    "        weight: The weight matrix.\n",
    "    \"\"\"\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x\n",
    "\n",
    "    tmp = 0.0\n",
    "    for i in range(bpg):\n",
    "        sA[ty, tx] = 0\n",
    "        sB[ty, tx] = 0\n",
    "        if y < input.shape[0] and (tx+i*TPB) < input.shape[1]:\n",
    "            sA[ty, tx] = input[y, tx + i * TPB]\n",
    "        if x < weight.shape[1] and (ty+i*TPB) < weight.shape[0]:\n",
    "            sB[ty, tx] = weight[ty + i * TPB, x]\n",
    "\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        cuda.syncthreads()\n",
    "    if y < output.shape[0] and x < output.shape[1]:\n",
    "        output[y, x] = tmp\n",
    "\n",
    "\n",
    "class NumbaLinearFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        \n",
    "        output = torch.empty(input.size(0), weight.size(0), device=input.device)\n",
    "        \n",
    "        threads_per_block = (TPB, TPB)\n",
    "        grid_y_max = max(input.shape[0], weight.shape[0])\n",
    "        grid_x_max = max(input.shape[1], weight.shape[1])\n",
    "\n",
    "        blocks_per_grid_x = math.ceil(grid_x_max / threads_per_block[0])\n",
    "        blocks_per_grid_y = math.ceil(grid_y_max / threads_per_block[1])\n",
    "\n",
    "        blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "        \n",
    "        linear_kernel[blocks_per_grid, threads_per_block](\n",
    "            input.detach(), output, weight.detach().T\n",
    "        )\n",
    "        \n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "\n",
    "\n",
    "class NumbaLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Performs a linear transformation on a tensor using Numba CUDA.\n",
    "\n",
    "    This class implements a linear transformation with configurable input and output features, and optional bias.\n",
    "    It leverages Numba CUDA for efficient GPU acceleration.\n",
    "\n",
    "    Args:\n",
    "        in_features (int): The number of input features.\n",
    "        out_features (int): The number of output features.\n",
    "        bias (bool, optional): Whether to use a bias term. Defaults to True.\n",
    "        custom_weight (torch.Tensor, optional): A custom weight tensor to use. Defaults to None.\n",
    "        custom_bias (torch.Tensor, optional): A custom bias tensor to use. Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 bias: bool = True,\n",
    "                 custom_weight = None,\n",
    "                 custom_bias = None) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return NumbaLinearFunction.apply(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826920a",
   "metadata": {
    "papermill": {
     "duration": 0.00924,
     "end_time": "2024-08-11T05:56:11.523817",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.514577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Numba ResNet9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31141bcc",
   "metadata": {},
   "source": [
    "Basing on developed modules, we will stack them together to get `ConvBlock` then is total `ResNet9` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "787b144a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:11.543665Z",
     "iopub.status.busy": "2024-08-11T05:56:11.543405Z",
     "iopub.status.idle": "2024-08-11T05:56:11.550267Z",
     "shell.execute_reply": "2024-08-11T05:56:11.549526Z"
    },
    "papermill": {
     "duration": 0.018876,
     "end_time": "2024-08-11T05:56:11.552160",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.533284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NumbaConvBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel: int = 3,\n",
    "                 stride: int = 1,\n",
    "                 padding: int = 1,\n",
    "                 pooling: bool = False,\n",
    "                 pooling_kernel: int = 4) -> None:\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            NumbaConv2d(in_channels, out_channels, kernel_size=kernel, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        if pooling:\n",
    "            self.conv.append(NumbaMaxPool2d(kernel_size=pooling_kernel))\n",
    "\n",
    "    def forward(self, X: Tensor):\n",
    "        return self.conv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f0d4b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:11.571881Z",
     "iopub.status.busy": "2024-08-11T05:56:11.571586Z",
     "iopub.status.idle": "2024-08-11T05:56:11.580318Z",
     "shell.execute_reply": "2024-08-11T05:56:11.579641Z"
    },
    "papermill": {
     "duration": 0.020736,
     "end_time": "2024-08-11T05:56:11.582130",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.561394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NumbaResNet9(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 num_classes: int,) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = NumbaConvBlock(in_channels=in_channels, out_channels=64)\n",
    "        self.conv2 = NumbaConvBlock(in_channels=64, out_channels=128, pooling=True)\n",
    "        \n",
    "        self.residual1 = nn.Sequential(\n",
    "            NumbaConvBlock(128, 128),\n",
    "            NumbaConvBlock(128, 128)\n",
    "        )\n",
    "\n",
    "        self.conv3 = NumbaConvBlock(in_channels=128, out_channels=256, pooling=True)\n",
    "        self.conv4 = NumbaConvBlock(in_channels=256, out_channels=512, pooling=True)\n",
    "        \n",
    "        self.residual2 = nn.Sequential(\n",
    "            NumbaConvBlock(512, 512),\n",
    "            NumbaConvBlock(512, 512)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            NumbaMaxPool2d(4),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=512, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.residual1(x) + x\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.residual2(x) + x\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef3d4e7",
   "metadata": {
    "papermill": {
     "duration": 0.009102,
     "end_time": "2024-08-11T05:56:11.600927",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.591825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ResNet9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "291f1d5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:11.622311Z",
     "iopub.status.busy": "2024-08-11T05:56:11.621592Z",
     "iopub.status.idle": "2024-08-11T05:56:11.628360Z",
     "shell.execute_reply": "2024-08-11T05:56:11.627654Z"
    },
    "papermill": {
     "duration": 0.019957,
     "end_time": "2024-08-11T05:56:11.630258",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.610301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel: int = 3,\n",
    "                 stride: int = 1,\n",
    "                 padding: int = 1,\n",
    "                 pooling: bool = False,\n",
    "                 pooling_kernel: int = 4) -> None:\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        if pooling:\n",
    "            self.conv.append(nn.MaxPool2d(kernel_size=pooling_kernel))\n",
    "\n",
    "    def forward(self, X: Tensor):\n",
    "        return self.conv(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a90ed92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:11.650451Z",
     "iopub.status.busy": "2024-08-11T05:56:11.650190Z",
     "iopub.status.idle": "2024-08-11T05:56:11.658939Z",
     "shell.execute_reply": "2024-08-11T05:56:11.658117Z"
    },
    "papermill": {
     "duration": 0.021088,
     "end_time": "2024-08-11T05:56:11.660855",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.639767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet9(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 num_classes: int,) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = ConvBlock(in_channels=in_channels, out_channels=64)\n",
    "        self.conv2 = ConvBlock(in_channels=64, out_channels=128, pooling=True)\n",
    "\n",
    "        self.residual1 = nn.Sequential(\n",
    "            ConvBlock(128, 128),\n",
    "            ConvBlock(128, 128)\n",
    "        )\n",
    "\n",
    "        self.conv3 = ConvBlock(in_channels=128, out_channels=256, pooling=True)\n",
    "        self.conv4 = ConvBlock(in_channels=256, out_channels=512, pooling=True)\n",
    "\n",
    "        self.residual2 = nn.Sequential(\n",
    "            ConvBlock(512, 512),\n",
    "            ConvBlock(512, 512)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=512, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.residual1(x) + x\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.residual2(x) + x\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18766b7e",
   "metadata": {
    "papermill": {
     "duration": 0.0094,
     "end_time": "2024-08-11T05:56:11.679735",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.670335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f173dd9",
   "metadata": {},
   "source": [
    "This code defines a custom PyTorch `Dataset` class called `PlantDiseaseDataset` for loading and processing plant disease images. It's designed to work with image datasets organized according to the `ImageFolder` convention, where each subdirectory represents a different disease class.\n",
    "\n",
    "The class provides the following functionalities:\n",
    "\n",
    "- **Initialization:** Takes the path to the image directory and an optional transformation function as input. If no transformation is provided, it defaults to converting images to PyTorch tensors using `transforms.ToTensor()`.\n",
    "- **Length:** Returns the total number of images in the dataset using `len(self.img_folder)`.\n",
    "- **Item Access:** Implements the `__getitem__` method to retrieve a single image and its corresponding label (disease class) from the dataset. It uses the `ImageFolder` object to handle image loading and transformation.\n",
    "\n",
    "This class simplifies the process of loading and preparing plant disease images for training and evaluation in a PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f420a1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:11.700664Z",
     "iopub.status.busy": "2024-08-11T05:56:11.699751Z",
     "iopub.status.idle": "2024-08-11T05:56:11.706453Z",
     "shell.execute_reply": "2024-08-11T05:56:11.705657Z"
    },
    "papermill": {
     "duration": 0.019239,
     "end_time": "2024-08-11T05:56:11.708393",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.689154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PlantDiseaseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for plant disease classification.\n",
    "\n",
    "    This class loads images from a specified directory and applies optional transformations.\n",
    "    It assumes the directory structure follows the ImageFolder convention, where each subdirectory\n",
    "    represents a different disease class.\n",
    "\n",
    "    If no transformations are provided (`transforms` is None), the class will convert the images\n",
    "    to PyTorch tensors by default.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the directory containing the plant disease images.\n",
    "        transforms (Callable, optional): A callable object (e.g., torchvision.transforms)\n",
    "            to apply to the images. Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 path: str,\n",
    "                 transform_function: Optional[Callable] = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        transform = transform_function or transforms.ToTensor()\n",
    "        self.img_folder = ImageFolder(path, transform=transform)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_folder)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
    "        return self.img_folder[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48304e88",
   "metadata": {
    "papermill": {
     "duration": 0.009575,
     "end_time": "2024-08-11T05:56:11.727775",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.718200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b3f1e",
   "metadata": {},
   "source": [
    "Calculates the accuracy of a model's predictions by comparing the argmax of the model's logits to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "580f797e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:11.748801Z",
     "iopub.status.busy": "2024-08-11T05:56:11.748139Z",
     "iopub.status.idle": "2024-08-11T05:56:11.752955Z",
     "shell.execute_reply": "2024-08-11T05:56:11.752101Z"
    },
    "papermill": {
     "duration": 0.017428,
     "end_time": "2024-08-11T05:56:11.754927",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.737499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred: Tensor, y: Tensor):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of a model's predictions.\n",
    "\n",
    "    Args:\n",
    "        y_pred (torch.Tensor): The model's unnormalized logits.\n",
    "        y (torch.Tensor): The true labels.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model.\n",
    "    \"\"\"\n",
    "    y_pred = torch.argmax(y_pred, 1)\n",
    "\n",
    "    return (y_pred == y).type(torch.float).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50400fc5",
   "metadata": {
    "papermill": {
     "duration": 0.009504,
     "end_time": "2024-08-11T05:56:11.774166",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.764662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99d71db0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:11.794922Z",
     "iopub.status.busy": "2024-08-11T05:56:11.794141Z",
     "iopub.status.idle": "2024-08-11T05:56:11.798568Z",
     "shell.execute_reply": "2024-08-11T05:56:11.797869Z"
    },
    "papermill": {
     "duration": 0.016542,
     "end_time": "2024-08-11T05:56:11.800332",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.783790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer: Optimizer):\n",
    "    \"\"\"\n",
    "    Returns the learning rate of the optimiz`er.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): The optimizer to get the learning rate from.\n",
    "\n",
    "    Returns:\n",
    "        float: The learning rate of the optimizer.\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450bfe8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T09:07:47.346808Z",
     "iopub.status.busy": "2024-07-09T09:07:47.346155Z",
     "iopub.status.idle": "2024-07-09T09:07:47.351254Z",
     "shell.execute_reply": "2024-07-09T09:07:47.350435Z",
     "shell.execute_reply.started": "2024-07-09T09:07:47.346775Z"
    },
    "papermill": {
     "duration": 0.009373,
     "end_time": "2024-08-11T05:56:11.819424",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.810051",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba820b",
   "metadata": {
    "papermill": {
     "duration": 0.009424,
     "end_time": "2024-08-11T05:56:11.838559",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.829135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb81fd0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:56:11.858926Z",
     "iopub.status.busy": "2024-08-11T05:56:11.858644Z",
     "iopub.status.idle": "2024-08-11T05:57:10.728507Z",
     "shell.execute_reply": "2024-08-11T05:57:10.727682Z"
    },
    "papermill": {
     "duration": 58.882639,
     "end_time": "2024-08-11T05:57:10.730878",
     "exception": false,
     "start_time": "2024-08-11T05:56:11.848239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)'\n",
    "\n",
    "train_dataset = PlantDiseaseDataset(data_path + '/train')\n",
    "val_dataset = PlantDiseaseDataset(data_path + '/valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22807f23",
   "metadata": {
    "papermill": {
     "duration": 0.010155,
     "end_time": "2024-08-11T05:57:10.751698",
     "exception": false,
     "start_time": "2024-08-11T05:57:10.741543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following code was used to test model with a overly small set of data. Please un-comment this code in right case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9edd9cab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:57:10.773199Z",
     "iopub.status.busy": "2024-08-11T05:57:10.772871Z",
     "iopub.status.idle": "2024-08-11T05:57:10.776810Z",
     "shell.execute_reply": "2024-08-11T05:57:10.775930Z"
    },
    "papermill": {
     "duration": 0.017115,
     "end_time": "2024-08-11T05:57:10.778727",
     "exception": false,
     "start_time": "2024-08-11T05:57:10.761612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = Subset(train_dataset, torch.linspace(0, len(train_dataset) - 1, 100).type(torch.int))\n",
    "# val_dataset = Subset(val_dataset, torch.linspace(0, len(val_dataset) - 1, 10).type(torch.int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e12a1",
   "metadata": {
    "papermill": {
     "duration": 0.009578,
     "end_time": "2024-08-11T05:57:10.798467",
     "exception": false,
     "start_time": "2024-08-11T05:57:10.788889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Data Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a09afe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:57:10.819595Z",
     "iopub.status.busy": "2024-08-11T05:57:10.819321Z",
     "iopub.status.idle": "2024-08-11T05:57:10.824932Z",
     "shell.execute_reply": "2024-08-11T05:57:10.823992Z"
    },
    "papermill": {
     "duration": 0.01812,
     "end_time": "2024-08-11T05:57:10.826847",
     "exception": false,
     "start_time": "2024-08-11T05:57:10.808727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f9109c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T09:23:49.389129Z",
     "iopub.status.busy": "2024-07-09T09:23:49.388572Z",
     "iopub.status.idle": "2024-07-09T09:23:49.395104Z",
     "shell.execute_reply": "2024-07-09T09:23:49.393962Z",
     "shell.execute_reply.started": "2024-07-09T09:23:49.389096Z"
    },
    "papermill": {
     "duration": 0.010431,
     "end_time": "2024-08-11T05:57:10.847436",
     "exception": false,
     "start_time": "2024-08-11T05:57:10.837005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Loss, Optimizer, and other essential stuffs for training model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79d89b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:57:10.868718Z",
     "iopub.status.busy": "2024-08-11T05:57:10.868371Z",
     "iopub.status.idle": "2024-08-11T05:57:11.245091Z",
     "shell.execute_reply": "2024-08-11T05:57:11.244160Z"
    },
    "papermill": {
     "duration": 0.389474,
     "end_time": "2024-08-11T05:57:11.247226",
     "exception": false,
     "start_time": "2024-08-11T05:57:10.857752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = NumbaResNet9(3, 38)\n",
    "model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4057a34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:57:11.269047Z",
     "iopub.status.busy": "2024-08-11T05:57:11.268752Z",
     "iopub.status.idle": "2024-08-11T05:57:11.274951Z",
     "shell.execute_reply": "2024-08-11T05:57:11.274085Z"
    },
    "papermill": {
     "duration": 0.01919,
     "end_time": "2024-08-11T05:57:11.276934",
     "exception": false,
     "start_time": "2024-08-11T05:57:11.257744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "max_lr = 0.01\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-3\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                          max_lr,\n",
    "                                          epochs=epochs,\n",
    "                                          steps_per_epoch=len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a689a29",
   "metadata": {
    "papermill": {
     "duration": 0.009866,
     "end_time": "2024-08-11T05:57:11.297067",
     "exception": false,
     "start_time": "2024-08-11T05:57:11.287201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Setup training monitor**\n",
    "\n",
    "We use Weights & Biases (W&B) to track our training progress. This allows us to monitor metrics like loss and accuracy over time, visualize training curves, and easily compare different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e58e0214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:57:11.318305Z",
     "iopub.status.busy": "2024-08-11T05:57:11.318040Z",
     "iopub.status.idle": "2024-08-11T05:57:13.356533Z",
     "shell.execute_reply": "2024-08-11T05:57:13.355692Z"
    },
    "papermill": {
     "duration": 2.051295,
     "end_time": "2024-08-11T05:57:13.358496",
     "exception": false,
     "start_time": "2024-08-11T05:57:11.307201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "wandb_api_key = user_secrets.get_secret(\"wandb_api\")\n",
    "wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d777cb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:57:13.381243Z",
     "iopub.status.busy": "2024-08-11T05:57:13.380952Z",
     "iopub.status.idle": "2024-08-11T05:57:30.124414Z",
     "shell.execute_reply": "2024-08-11T05:57:30.123519Z"
    },
    "papermill": {
     "duration": 16.757362,
     "end_time": "2024-08-11T05:57:30.126724",
     "exception": false,
     "start_time": "2024-08-11T05:57:13.369362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbuitanphuong10c13\u001b[0m (\u001b[33mbtp712\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240811_055713-ryggrnen\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mNumba ResNet9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/btp712/Plant%20Diseases%20Identification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/btp712/Plant%20Diseases%20Identification/runs/ryggrnen\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project='Plant Diseases Identification',\n",
    "    name='Numba ResNet9',\n",
    "    config={\n",
    "        'epoch': 3,\n",
    "        'batch_size': 128\n",
    "    },\n",
    ")\n",
    "\n",
    "STEP_PER_LOG = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb55d6a",
   "metadata": {
    "papermill": {
     "duration": 0.011271,
     "end_time": "2024-08-11T05:57:30.150036",
     "exception": false,
     "start_time": "2024-08-11T05:57:30.138765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e2bb011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:57:30.174611Z",
     "iopub.status.busy": "2024-08-11T05:57:30.174308Z",
     "iopub.status.idle": "2024-08-11T06:36:53.560505Z",
     "shell.execute_reply": "2024-08-11T06:36:53.559572Z"
    },
    "papermill": {
     "duration": 2363.400952,
     "end_time": "2024-08-11T06:36:53.562488",
     "exception": false,
     "start_time": "2024-08-11T05:57:30.161536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Train : 100%|| 1099/1099 [13:15<00:00,  1.38it/s, loss=0.691, accuracy=0.8]\n",
      " Eval  : 100%|| 275/275 [01:56<00:00,  2.35it/s, loss=1.01, accuracy=0.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Train : 100%|| 1099/1099 [10:38<00:00,  1.72it/s, loss=0.111, accuracy=0.964]\n",
      " Eval  : 100%|| 275/275 [01:15<00:00,  3.64it/s, loss=0.0481, accuracy=0.984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Train : 100%|| 1099/1099 [10:52<00:00,  1.69it/s, loss=0.019, accuracy=0.994]\n",
      " Eval  : 100%|| 275/275 [01:24<00:00,  3.24it/s, loss=0.0126, accuracy=0.996]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "batch_count, num_log = 0, 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_running_loss, train_acc = 0.0, 0.0\n",
    "    logging_dict = {}\n",
    "    \n",
    "    # Train\n",
    "    model.train() \n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    train_loop = tqdm(train_dataloader, desc=f'{\"Train\":^7}', leave=True)\n",
    "    for i, data in enumerate(train_loop):\n",
    "        # load data to cuda\n",
    "        X, y = (_.cuda() for _ in data)\n",
    "        \n",
    "        # compute y_pred\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # loss\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient clipping\n",
    "        nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # update lr\n",
    "        scheduler.step()\n",
    "        \n",
    "        # update loss\n",
    "        train_running_loss += loss.item()\n",
    "        train_acc += accuracy(y_pred, y)\n",
    "        \n",
    "        logging_dict = {'loss': train_running_loss / (i + 1),\n",
    "                        'accuracy': train_acc / (i + 1)}\n",
    "        \n",
    "        # update progress bar\n",
    "        train_loop.set_postfix(logging_dict)\n",
    "        \n",
    "        # wandb logging\n",
    "        batch_count += 1\n",
    "        if batch_count // STEP_PER_LOG == num_log or i == len(train_dataloader) - 1:\n",
    "            logging_dict['epoch'] = batch_count / len(train_dataloader)\n",
    "            logging_dict['learning rate'] = get_lr(optimizer)\n",
    "            \n",
    "            wandb.log({f'train/{k}': v for k, v in logging_dict.items()}, step=batch_count)\n",
    "            \n",
    "            num_log += 1\n",
    "            \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    val_running_loss, val_acc = 0.0, 0.0\n",
    "    val_loop = tqdm(val_dataloader, desc=f\"{'Eval':^7}\", leave=True)\n",
    "    for i, data in enumerate(val_loop):\n",
    "        X, y = (_.to(device) for _ in data)\n",
    "\n",
    "        y_pred = model(X)\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        val_running_loss += loss.item()\n",
    "        val_acc += accuracy(y_pred, y)\n",
    "\n",
    "        logging_dict = {\n",
    "            'loss': val_running_loss / (i + 1),\n",
    "            'accuracy': val_acc / (i + 1)\n",
    "        }\n",
    "        val_loop.set_postfix(logging_dict)\n",
    "\n",
    "    wandb.log({\n",
    "        'train/epoch': epoch + 1,\n",
    "        'eval/loss': val_running_loss / len(val_dataloader),\n",
    "        'eval/accuracy': val_acc / len(val_dataloader)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0440fa8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T06:36:54.942954Z",
     "iopub.status.busy": "2024-08-11T06:36:54.942197Z",
     "iopub.status.idle": "2024-08-11T06:36:54.993073Z",
     "shell.execute_reply": "2024-08-11T06:36:54.992267Z"
    },
    "papermill": {
     "duration": 0.715869,
     "end_time": "2024-08-11T06:36:54.995189",
     "exception": false,
     "start_time": "2024-08-11T06:36:54.279320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.module, 'numba_resnet9.pt')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 78313,
     "sourceId": 182633,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2455.818448,
   "end_time": "2024-08-11T06:36:58.389404",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-11T05:56:02.570956",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
